
## Push-Pull Pareto Frontiers

In this project, we extend the gain-vs-pain pareto frontier experiments as follows:
1. We compare push, pull, and push-pull user models for system access using the pareto frontier for assessing utility of the systems.
2. We integrate nuggets vs clusters with this analysis for TS 13 and TS 14
3. ? We also experiment with different presentation/interaction models (chrono, ranked, reverse-chrono updates)
4. ? latency in a 3D pareto front
5. ? We experiment with differing levels of verbosity (binary vs true vs estimated); true verbosity may vastly increase pain
6. ? what about including vs. not including sentences in the pool

### TODOs
- [DONE] convert code to python 3
- [DONE] run code and check that results are same as previous
- [NA] write in code for only pull scenario
    - [DONE] generate results
- [NA] write in code for push-pull spectrum
    - [DONE] generate results 
- use cluster qrels for TS 13 and TS 14
    - generate results
- [ONGOING] visualization of pareto-frontiers
    - ? can we visualize performance over the entire spectrum  
    - ? small multiples plots
    - [DONE] 3D plots
    - comparison plots: [A]
        - systems' gain lines for 3 interaction models
        - systems' pain lines for 3 interaction models
        - significant differences if any for gain|pain with different interaction models
- interface models
    - chrono|reverse|ranked
    - comparison plots [repeat [A]]
    - ? does chrono|reverse interface result in improved gain|pain characterstics over ranked interfaces
- we have interaction, user, interface models:
    - ? what is best for which kind of user
    - ? does the pareto frontier change significantly? if yes, then in how many cases?
    

    
#### Software requirements
- (Anaconda Python 3.6)[https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh]

<span style="color:red"> Note: ```cython``` is needed for processing over complex user and interface models. 
Complex interface models necessitate looping over all submitted updates; looping over lists is very slow in pure Python. </span>

### Data Requirements

- ```data/```: sibling folder to this folder
- ```├── mb-2015/```: **Microblog Track (MB) 2015**
- ```│   ├── qrels/```: MB 2015 qrels
- ```│   ├── submitted-runs-scenario-A/```: systems' outputs submitted to MB 2015 (download from [TREC](http://trec.nist.gov))
- ```├── rts-2016/```: **Real Time Summarization (RTS) 2016**
- ```│   ├── qrels/```: RTS 2016 qrels
- ```│   └── submitted-runs-scenario-A/```: systems' outputs submitted to RT 2016 (download from [TREC](http://trec.nist.gov))
- ```├── ts-2013/```: **Temporal Summarization (TS) Track 2013** 
- ```│   ├── qrels/```: TS 2013 qrels
- ```│   ├── submitted-runs/```: systems' outputs submitted to TS 2013 (download from [TREC](http://trec.nist.gov))
- ```│   └── update-lengths/```: lengths of updates submitted to TS 2013 (download from [here]- (https://cs.uwaterloo.ca/~gbaruah/ts-2013-update-lengths.html))
- ```└── ts-2014/```: **Temporal Summarization Track 2014**
- ```    ├── qrels/```: TS 2014 qrels
- ```    ├── submitted-runs/```: systems' outputs submitted to TS 2014 (download from [TREC](http://trec.nist.gov))
- ```    └── update-lengths/```: (download from [here](https://cs.uwaterloo.ca/~gbaruah/ts-2014-update-lengths.html))


### Code Layout

<span style="color:red">The code layout will need to change</span>


```msu-2016``` : **Main** codebase <br>
```├── Readme.md``` : This Readme.md <br>
```├── modeled_stream_utility.py``` : **main script** <br>
```├── nugget.py``` : nugget class for Temporal Summarization tracks <br>
```├── update.py``` : update class for sentences submitted to Temporal Summarization tracks <br>
```├── get_query_durations.py``` : extracts start and end timestamps for query durations from the tracks' topics.xml file <br>
```├── probability_distributions.py``` : base classes for probability distributions <br>
```├── population_model.py``` : user population model <br>
```├── user_model.py``` : user behavior model <br>
```├── user_interface_model.py``` : user interface models <br>
```├── utils.py``` : Utility functions  <br>

```Cython```-ic files for complex user interface models <br>
```├── cython_computations.pyx``` : defines a custom heap class and computes msu for ranked interfaces  <br>
```├── setup.py``` : builds ```cython_computations``` library for importing into python code  <br>

Files for comparing msu-2016 and sigir-2015 codebases (see [codebase-comparison](#Codebase-comparison))  <br>
```├── modeled_stream_utility_with_time_trails.py```: script to evaluate using time trails made by R (see [sigir-2015/Readme.md](../sigir-2015/Readme.md) ) <br>
```└── gen-pythonic-time-trails.py```: script to compute MSU given user time-trails generated by [sigir-2015/generate.time.trails.R](../sigir-2015/generate.time.trails.R)  <br>


### Running the Evaluation

check out ```run-evaluations.sh``` to get to know the arguments to ```modeled_stream_utility_push-ranked_order.py``` , the main eval script.
